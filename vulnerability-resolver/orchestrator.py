#!/usr/bin/env python3
import argparse
import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional

from parse_sarif import SARIFParser, VulnerabilityBatcher, find_most_recent_sarif
from devin_api_client import DevinAPIClient


def setup_logging(log_file: str = "resolver.log") -> logging.Logger:
    logger = logging.getLogger("vulnerability_resolver")
    logger.setLevel(logging.INFO)
    
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


class VulnerabilityResolver:
    def __init__(self, repo_url: str, sarif_path: Optional[str] = None,
                 batch_size: int = 4, dry_run: bool = False,
                 max_batches: Optional[int] = None, 
                 branch: Optional[str] = None,
                 session_timeout: int = 1800,
                 poll_interval: int = 30,
                 logger: Optional[logging.Logger] = None):
        self.repo_url = repo_url
        self.sarif_path = sarif_path
        self.batch_size = batch_size
        self.dry_run = dry_run
        self.max_batches = max_batches
        self.branch = branch
        self.session_timeout = session_timeout
        self.poll_interval = poll_interval
        self.logger = logger or setup_logging()
        
        if not self.sarif_path:
            self.sarif_path = find_most_recent_sarif()
            if self.sarif_path:
                self.logger.info(f"Auto-detected SARIF file: {self.sarif_path}")
            else:
                raise ValueError("No SARIF file found in codeql-results/ directory")
        
        if not Path(self.sarif_path).exists():
            raise FileNotFoundError(f"SARIF file not found: {self.sarif_path}")
        
        self.devin_client = None
        if not self.dry_run:
            try:
                self.devin_client = DevinAPIClient()
                self.logger.info("Initialized Devin API client")
            except ValueError as e:
                self.logger.error(f"Failed to initialize Devin API client: {e}")
                raise
    
    def run(self) -> Dict[str, Any]:
        self.logger.info("=" * 80)
        self.logger.info("Starting Vulnerability Resolution Process")
        self.logger.info("=" * 80)
        self.logger.info(f"SARIF file: {self.sarif_path}")
        self.logger.info(f"Repository: {self.repo_url}")
        self.logger.info(f"Target branch: {self.branch or 'default'}")
        self.logger.info(f"Batch size: {self.batch_size}")
        self.logger.info(f"Dry run: {self.dry_run}")
        self.logger.info(f"Max batches: {self.max_batches or 'unlimited'}")
        self.logger.info("=" * 80)
        
        self.logger.info("Parsing SARIF file...")
        parser = SARIFParser(self.sarif_path)
        vulnerabilities = parser.parse()
        self.logger.info(f"Found {len(vulnerabilities)} unique vulnerabilities")
        
        if not vulnerabilities:
            self.logger.warning("No vulnerabilities found in SARIF file")
            return {
                "status": "completed",
                "vulnerabilities_found": 0,
                "batches_created": 0,
                "batches_processed": 0,
                "successful_batches": 0,
                "failed_batches": 0,
                "batches": []
            }
        
        self.logger.info("Creating batches...")
        batcher = VulnerabilityBatcher(batch_size=self.batch_size)
        batches = batcher.create_batches(vulnerabilities)
        self.logger.info(f"Created {len(batches)} batches")
        
        if self.max_batches:
            batches = batches[:self.max_batches]
            self.logger.info(f"Limited to {len(batches)} batches (max_batches={self.max_batches})")
        
        self._save_batches(batches)
        
        if self.dry_run:
            self.logger.info("=" * 80)
            self.logger.info("DRY RUN MODE - No API calls will be made")
            self.logger.info("=" * 80)
            self._print_batch_summary(batches)
            return {
                "status": "dry_run_completed",
                "vulnerabilities_found": len(vulnerabilities),
                "batches_created": len(batches),
                "batches_processed": 0,
                "successful_batches": 0,
                "failed_batches": 0,
                "batches": batches
            }
        
        self.logger.info("=" * 80)
        self.logger.info("Processing batches with Devin API...")
        self.logger.info("=" * 80)
        
        results = self._process_batches(batches)
        
        self.logger.info("=" * 80)
        self.logger.info("Vulnerability Resolution Complete")
        self.logger.info("=" * 80)
        self.logger.info(f"Total vulnerabilities: {len(vulnerabilities)}")
        self.logger.info(f"Batches created: {len(batches)}")
        self.logger.info(f"Batches processed: {results['batches_processed']}")
        self.logger.info(f"Successful: {results['successful_batches']}")
        self.logger.info(f"Failed: {results['failed_batches']}")
        self.logger.info("=" * 80)
        
        return results
    
    def _process_batches(self, batches: List[Dict[str, Any]]) -> Dict[str, Any]:
        successful = 0
        failed = 0
        processed_batches = []
        
        for i, batch in enumerate(batches, 1):
            self.logger.info(f"\n{'=' * 80}")
            self.logger.info(f"Processing Batch {i}/{len(batches)}")
            self.logger.info(f"{'=' * 80}")
            self.logger.info(f"PR Title: {batch['pr_title']}")
            self.logger.info(f"Bucket: {batch['bucket']}")
            self.logger.info(f"Vulnerabilities: {batch['summary']['count']}")
            self.logger.info(f"Files: {', '.join(batch['summary']['files'])}")
            
            try:
                session_id = self.devin_client.create_vulnerability_fix_session(
                    repo_url=self.repo_url,
                    batch=batch,
                    branch=self.branch
                )
                
                if not session_id:
                    self.logger.error(f"Failed to create Devin session for batch {i}")
                    failed += 1
                    batch['status'] = 'failed'
                    batch['error'] = 'Failed to create session'
                    processed_batches.append(batch)
                    continue
                
                batch['session_id'] = session_id
                self.logger.info(f"Created session: {session_id}")
                
                self.logger.info("Waiting for session to complete...")
                session_data = self.devin_client.wait_for_completion(
                    session_id,
                    max_wait_seconds=self.session_timeout,
                    poll_interval=self.poll_interval
                )
                
                if not session_data:
                    self.logger.error(f"Session {session_id} did not complete successfully")
                    failed += 1
                    batch['status'] = 'failed'
                    batch['error'] = 'Session timeout or error'
                    processed_batches.append(batch)
                    continue
                
                pr_url = self.devin_client.extract_pr_url(session_data)
                
                if pr_url:
                    self.logger.info(f"âœ“ PR created: {pr_url}")
                    batch['pr_url'] = pr_url
                    batch['status'] = 'success'
                    successful += 1
                else:
                    self.logger.warning(f"Session completed but no PR URL found")
                    batch['status'] = 'completed_no_pr'
                    batch['session_data'] = session_data
                    successful += 1
                
                processed_batches.append(batch)
                
            except Exception as e:
                self.logger.error(f"Error processing batch {i}: {e}")
                failed += 1
                batch['status'] = 'failed'
                batch['error'] = str(e)
                processed_batches.append(batch)
                continue
        
        return {
            "status": "completed",
            "vulnerabilities_found": sum(b['summary']['count'] for b in batches),
            "batches_created": len(batches),
            "batches_processed": len(processed_batches),
            "successful_batches": successful,
            "failed_batches": failed,
            "batches": processed_batches
        }
    
    def _save_batches(self, batches: List[Dict[str, Any]]) -> None:
        output_file = "batches.json"
        with open(output_file, 'w') as f:
            json.dump(batches, f, indent=2)
        self.logger.info(f"Saved batch metadata to {output_file}")
    
    def _print_batch_summary(self, batches: List[Dict[str, Any]]) -> None:
        for i, batch in enumerate(batches, 1):
            self.logger.info(f"\nBatch {i}:")
            self.logger.info(f"  ID: {batch['batch_id']}")
            self.logger.info(f"  Bucket: {batch['bucket']}")
            self.logger.info(f"  PR Title: {batch['pr_title']}")
            self.logger.info(f"  Vulnerabilities: {batch['summary']['count']}")
            self.logger.info(f"  Files: {', '.join(batch['summary']['files'])}")
            self.logger.info(f"  Issue Types: {', '.join(batch['summary']['issue_types'])}")


def main():
    parser = argparse.ArgumentParser(
        description="Automated Vulnerability Resolution System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python orchestrator.py --repo-url https://github.com/user/repo --dry-run
  
  python orchestrator.py --repo-url https://github.com/user/repo --sarif path/to/results.sarif
  
  python orchestrator.py --repo-url https://github.com/user/repo --max-batches 3
  
  python orchestrator.py --repo-url https://github.com/user/repo --batch-size 3
        """
    )
    
    parser.add_argument(
        '--repo-url',
        required=True,
        help='GitHub repository URL (e.g., https://github.com/user/repo)'
    )
    
    parser.add_argument(
        '--sarif',
        help='Path to SARIF file (defaults to most recent in codeql-results/)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='Number of vulnerabilities per batch (default: 4)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Output planned batches without calling Devin API'
    )
    
    parser.add_argument(
        '--max-batches',
        type=int,
        help='Maximum number of batches to process (for testing)'
    )
    
    parser.add_argument(
        '--branch',
        help='Target branch for PRs (defaults to repo default branch)'
    )
    
    parser.add_argument(
        '--log-file',
        default='resolver.log',
        help='Log file path (default: resolver.log)'
    )
    
    parser.add_argument(
        '--session-timeout',
        type=int,
        default=1800,
        help='Maximum seconds to wait for each session (default: 1800 = 30 minutes)'
    )
    
    parser.add_argument(
        '--poll-interval',
        type=int,
        default=30,
        help='Seconds between status polls (default: 30)'
    )
    
    args = parser.parse_args()
    
    logger = setup_logging(args.log_file)
    
    try:
        resolver = VulnerabilityResolver(
            repo_url=args.repo_url,
            sarif_path=args.sarif,
            batch_size=args.batch_size,
            dry_run=args.dry_run,
            branch=args.branch,
            max_batches=args.max_batches,
            session_timeout=args.session_timeout,
            poll_interval=args.poll_interval,
            logger=logger
        )
        
        results = resolver.run()
        
        results_file = "results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        logger.info(f"\nResults saved to {results_file}")
        
        if results['status'] == 'dry_run_completed':
            sys.exit(0)
        elif results['failed_batches'] > 0:
            logger.warning(f"Completed with {results['failed_batches']} failed batches")
            sys.exit(1)
        else:
            sys.exit(0)
            
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()
